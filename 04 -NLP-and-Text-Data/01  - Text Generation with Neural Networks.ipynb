{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4356a90b",
   "metadata": {},
   "source": [
    "# Text Generation with Neural Networks\n",
    "\n",
    "In this notebook we will create a network that can generate text, here we show it being done character by character. Very awesome write up on this here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "We organized the process into \"steps\" so you can easily follow along with your own data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6acea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE COLLAB USERS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b1282d",
   "metadata": {
    "id": "b0b1282d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb6dae",
   "metadata": {
    "id": "6eeb6dae"
   },
   "source": [
    "## 1. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1717367",
   "metadata": {},
   "source": [
    "You can grab any free text you want from here: https://www.gutenberg.org/\n",
    "\n",
    "We'll choose all of shakespeare's works (which we have already downloaded for you), mainly for two reasons:\n",
    "\n",
    "1. Its a large corpus of text, its usually recommended you have at least a source of 1 million characters total to get realistic text generation.\n",
    "\n",
    "\n",
    "2. It has a very distinctive style. Since the text data uses old style english and is formatted in the style of a stage play, it will be very obvious to us if the model is able to reproduce similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd6cd92",
   "metadata": {
    "id": "6dd6cd92"
   },
   "outputs": [],
   "source": [
    "path_to_file = '/content/shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3e84df",
   "metadata": {
    "id": "0a3e84df"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file, mode='r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033924cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "033924cb",
    "outputId": "1e45a4da-b30c-44e0-8d4a-b1c94d54ed83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e56d37",
   "metadata": {
    "id": "59e56d37"
   },
   "source": [
    "## <a> Unique Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730bacc4",
   "metadata": {
    "id": "730bacc4"
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3936e258",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3936e258",
    "outputId": "266a6c72-8237-4434-c166-af450c694899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e47c10",
   "metadata": {
    "id": "33e47c10"
   },
   "source": [
    "---------------\n",
    "--------\n",
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0e75e",
   "metadata": {
    "id": "0fd0e75e"
   },
   "source": [
    "## <a>* Text Vectorization\n",
    "    We know a neural network can't take in the raw string data, we need to assign numbers to each character. Let's create two dictionaries that can go from numeric index to character and character to numeric index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5aabc5",
   "metadata": {
    "id": "ec5aabc5"
   },
   "source": [
    "### <i><a>- Character to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff40356c",
   "metadata": {
    "id": "ff40356c"
   },
   "outputs": [],
   "source": [
    "char_to_index = {char : index for index, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1426b442",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1426b442",
    "outputId": "8527b8f8-18a9-402b-d575-5c5580418b54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e89e1c78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e89e1c78",
    "outputId": "f8e3ae4f-dd61-4bfe-fc18-4b361466dab7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9191d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e9191d0",
    "outputId": "d02424ba-6b69-479c-bdb8-496d3766abb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5019110",
   "metadata": {
    "id": "d5019110"
   },
   "source": [
    "### <i><a> - index to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ba988e",
   "metadata": {
    "id": "18ba988e"
   },
   "outputs": [],
   "source": [
    "index_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fcd21cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fcd21cb",
    "outputId": "f6f27fa2-e5fe-41d9-b96d-4612834394ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '|'], dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fed85c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fed85c8",
    "outputId": "e2a28229-e62e-4b71-e023-a6fe38d7a540"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da1294ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "da1294ec",
    "outputId": "bb1f3b19-00a3-4c21-d5d1-747c2521b90e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a74145",
   "metadata": {
    "id": "b4a74145"
   },
   "source": [
    "### <i><a> - Encoding the character of the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35363b70",
   "metadata": {
    "id": "35363b70"
   },
   "outputs": [],
   "source": [
    "encoding_text = np.array([char_to_index[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8deb081",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8deb081",
    "outputId": "a4b81288-17ee-4bf1-fd81-df59400dea3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ...,  1,  1, 39])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c05852",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55c05852",
    "outputId": "68b98946-1419-41e2-b06c-9ea74578d72c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145728,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb3ad96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acb3ad96",
    "outputId": "1a37dfae-5bbc-471d-e2d6-5c26a392f752"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52dc7f",
   "metadata": {
    "id": "0b52dc7f"
   },
   "source": [
    "### <i><a> Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b129f2fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b129f2fe",
    "outputId": "a2c8445b-7cb9-4a24-b012-f72b95dfa625"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n                   '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = text[:20]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467796c",
   "metadata": {
    "id": "d467796c"
   },
   "source": [
    "------\n",
    "------\n",
    "## 3. Creating Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16482e6",
   "metadata": {},
   "source": [
    "Overall what we are trying to achieve is to have the model predict the next highest probability character given a historical sequence of characters. Its up to us (the user) to choose how long that historic sequence. Too short a sequence and we don't have enough information (e.g. given the letter \"a\" , what is the next character) , too long a sequence and training will take too long and most likely overfit to sequence characters that are irrelevant to characters farther out. While there is no correct sequence length choice, you should consider the text itself, how long normal phrases are in it, and a reasonable idea of what characters/words are relevant to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774039e",
   "metadata": {
    "id": "7774039e"
   },
   "source": [
    "###  <i><a> First we need understanding the text we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cbad4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cbad4bb",
    "outputId": "ac395bc1-9c52-4f0c-bff9-356d8981d024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c86c0a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c86c0a6",
    "outputId": "89f51193-9ae2-44c5-c5ab-a1db25a3c0ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'From fairest creatures we desire increase,'\n",
    "len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f18858d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f18858d3",
    "outputId": "6e9cde6d-fd5e-48af-c8ad-14cbc742ab26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = '''\n",
    "  From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\n",
    "'''\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435385b",
   "metadata": {
    "id": "4435385b"
   },
   "source": [
    "### <i><a> Traning Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9d59a",
   "metadata": {},
   "source": [
    "The actual text data will be the text sequence shifted one character forward. For example:\n",
    "\n",
    "Sequence In: \"Hello my nam\" Sequence Out: \"ello my name\"\n",
    "\n",
    "We can use the `tf.data.Dataset.from_tensor_slices` function to convert a text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af129de6",
   "metadata": {
    "id": "af129de6"
   },
   "outputs": [],
   "source": [
    "sequence_length = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5d4d86e",
   "metadata": {
    "id": "b5d4d86e"
   },
   "outputs": [],
   "source": [
    "# calculate how many sequence for the whole text,because of 0 indeing we will add 1\n",
    "total_num_sequence = len(text) // (sequence_length + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82e75d9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82e75d9a",
    "outputId": "1e07ca9e-18ca-4cbb-deba-2ad866b6ccf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25997"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a1497",
   "metadata": {
    "id": "215a1497"
   },
   "source": [
    "### <i><a> Create Training Sequences (encoding_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df11b679",
   "metadata": {
    "id": "df11b679"
   },
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoding_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09104da4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09104da4",
    "outputId": "8f2e5f5d-26ae-48f9-f396-4eb1f50c87f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d94ce5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d94ce5c",
    "outputId": "a21d64fa-df69-4973-8464-2cdc7091dcbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "f\n",
      "a\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "s\n",
      "i\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "b\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "u\n",
      "t\n",
      "y\n",
      "'\n",
      "s\n",
      " \n",
      "r\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "i\n",
      "p\n",
      "e\n",
      "r\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "H\n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      "m\n",
      "o\n",
      "r\n",
      "y\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "b\n",
      "r\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "e\n",
      "y\n",
      "e\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "e\n",
      "e\n",
      "d\n",
      "'\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "l\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "'\n",
      "s\n",
      " \n",
      "f\n",
      "l\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "-\n",
      "s\n",
      "u\n",
      "b\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "f\n",
      "u\n",
      "e\n",
      "l\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "M\n",
      "a\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "a\n",
      "m\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "u\n",
      "n\n",
      "d\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "l\n",
      "i\n",
      "e\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "e\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "s\n",
      "w\n",
      "e\n",
      "e\n",
      "t\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      "o\n",
      " \n",
      "c\n",
      "r\n",
      "u\n",
      "e\n",
      "l\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "'\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "e\n",
      "s\n",
      "h\n",
      " \n",
      "o\n",
      "r\n",
      "n\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "a\n",
      "l\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "g\n",
      "a\n",
      "u\n",
      "d\n",
      "y\n",
      " \n",
      "s\n",
      "p\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "W\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "b\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "# create training sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoding_text)\n",
    "\n",
    "for item in char_dataset.take(500):\n",
    "    print(index_to_char[item.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b894017",
   "metadata": {},
   "source": [
    "The **batch** method converts these individual character calls into sequences we can feed in as a batch. We use seq_len+1 because of zero indexing. Here is what drop_remainder means:\n",
    "\n",
    "drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a05cab",
   "metadata": {
    "id": "11a05cab"
   },
   "source": [
    "### <i><a> Creating Sequence Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e450d6b4",
   "metadata": {
    "id": "e450d6b4"
   },
   "outputs": [],
   "source": [
    "sequence = char_dataset.batch(sequence_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54d8ad0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54d8ad0e",
    "outputId": "3e16c78a-50f9-4319-a45b-2b0238229259"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(121,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60732c6a",
   "metadata": {},
   "source": [
    "Now that we have our sequences, we will perform the following steps for each one to create our target text sequences:\n",
    "\n",
    "1. Grab the input text sequence\n",
    "2. Assign the target text sequence as the input text sequence shifted by one step forward\n",
    "3. Group them together as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2faf1a2",
   "metadata": {
    "id": "f2faf1a2"
   },
   "outputs": [],
   "source": [
    "def create_sequence_target(seq):\n",
    "    input_text = seq[:-1] # hello my nam\n",
    "    target_text = seq[1:] # ello my name\n",
    "    return(input_text, target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e62401eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e62401eb",
    "outputId": "79e74050-d1af-4cf4-9d55-727565c121e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(120,), dtype=tf.int64, name=None), TensorSpec(shape=(120,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sequence.map(create_sequence_target)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a2c3685",
   "metadata": {
    "id": "3a2c3685"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b879f405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b879f405",
    "outputId": "56b029bd-0fd7-44dc-f932-0b3a90657d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print(input_text.numpy())\n",
    "    print(''.join(index_to_char[input_text.numpy()]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(target_text.numpy())\n",
    "    print(''.join(index_to_char[target_text.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23532c24",
   "metadata": {
    "id": "23532c24"
   },
   "source": [
    "### <i><a> Generating Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41856af",
   "metadata": {},
   "source": [
    "Now that we have the actual sequences, we will create the batches, we want to shuffle these sequences into a random order, so the model doesn't overfit to any section of the text, but can instead generate characters given any seed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63d49baa",
   "metadata": {
    "id": "63d49baa"
   },
   "outputs": [],
   "source": [
    "# batch_size\n",
    "batch_size = 128\n",
    "\n",
    "buffer_size = 1000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c13396bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c13396bd",
    "outputId": "005fd310-072b-4dc6-92ba-e297ec407330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(128, 120), dtype=tf.int64, name=None), TensorSpec(shape=(128, 120), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a9bd55b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a9bd55b",
    "outputId": "caa76b74-2bfe-4f1d-fed4-ac064cf62390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01912080",
   "metadata": {
    "id": "01912080"
   },
   "source": [
    "------\n",
    "-----\n",
    "## 4. Creating The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161941c",
   "metadata": {},
   "source": [
    "We will use an LSTM based model with a few extra features, including an embedding layer to start off with and two LSTM layers. We based this model architecture off the <a href=\"https://deepmoji.mit.edu/\">DeepMoji</a> and the original source code can be found <a href ='https://github.com/bfelbo/DeepMoji'>here</a>.\n",
    "\n",
    "The embedding layer will serve as the input layer, which essentially creates a lookup table that maps the numbers indices of each character to a vector with \"embedding dim\" number of dimensions. As you can imagine, the larger this embedding size, the more complex the training. This is similar to the idea behind word2vec, where words are mapped to some n-dimensional space. Embedding before feeding straight into the LSTM usually leads to more realisitic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a8c40f8",
   "metadata": {
    "id": "5a8c40f8"
   },
   "outputs": [],
   "source": [
    "# length of the vocabulary in char\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# the embeding dimension\n",
    "embed_dim = 1000\n",
    "\n",
    "# numbers of RNN units\n",
    "rnn_neurons  = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36f5943f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36f5943f",
    "outputId": "b4fd6c85-443d-4b51-dcd0-397adf6b15e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "840bbad2",
   "metadata": {
    "id": "840bbad2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3bc33",
   "metadata": {
    "id": "64d3bc33"
   },
   "source": [
    "### <i><a> Setting up loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7471619",
   "metadata": {},
   "source": [
    "Now let's create a function that easily adapts to different variables as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb5d184e",
   "metadata": {
    "id": "cb5d184e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eaed2bc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaed2bc7",
    "outputId": "a7ec64fb-21bd-4024-84ba-e2a4036bafd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_categorical_crossentropy in module keras.src.losses:\n",
      "\n",
      "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1, ignore_class=None)\n",
      "    Computes the sparse categorical crossentropy loss.\n",
      "    \n",
      "    Standalone usage:\n",
      "    \n",
      "    >>> y_true = [1, 2]\n",
      "    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
      "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
      "    >>> assert loss.shape == (2,)\n",
      "    >>> loss.numpy()\n",
      "    array([0.0513, 2.303], dtype=float32)\n",
      "    \n",
      "    >>> y_true = [[[ 0,  2],\n",
      "    ...            [-1, -1]],\n",
      "    ...           [[ 0,  2],\n",
      "    ...            [-1, -1]]]\n",
      "    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n",
      "    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n",
      "    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n",
      "    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n",
      "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
      "    ...   y_true, y_pred, ignore_class=-1)\n",
      "    >>> loss.numpy()\n",
      "    array([[[2.3841855e-07, 2.3841855e-07],\n",
      "            [0.0000000e+00, 0.0000000e+00]],\n",
      "           [[2.3841855e-07, 6.9314730e-01],\n",
      "            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n",
      "    \n",
      "    Args:\n",
      "      y_true: Ground truth values.\n",
      "      y_pred: The predicted values.\n",
      "      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "        default, we assume that `y_pred` encodes a probability distribution.\n",
      "      axis: Defaults to -1. The dimension along which the entropy is\n",
      "        computed.\n",
      "      ignore_class: Optional integer. The ID of a class to be ignored during\n",
      "        loss computation. This is useful, for example, in segmentation\n",
      "        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n",
      "        maps. By default (`ignore_class=None`), all classes are considered.\n",
      "    \n",
      "    Returns:\n",
      "      Sparse categorical crossentropy loss value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "343392c6",
   "metadata": {
    "id": "343392c6"
   },
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32fdb4",
   "metadata": {
    "id": "0b32fdb4"
   },
   "source": [
    "https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bfed723",
   "metadata": {
    "id": "1bfed723"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\n",
    "\n",
    "    model.add(GRU(rnn_neurons,\n",
    "                          return_sequences=True,\n",
    "                          stateful=True,\n",
    "                          recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dense(vocab_size))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76b3e95e",
   "metadata": {
    "id": "76b3e95e"
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size = vocab_size,\n",
    "                     embed_dim = embed_dim,\n",
    "                     rnn_neurons = rnn_neurons,\n",
    "                     batch_size  = batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2f92cf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2f92cf1",
    "outputId": "57167bd8-d2fc-4b26-8599-a04629571150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (128, None, 1000)         83000     \n",
      "                                                                 \n",
      " gru (GRU)                   (128, None, 1026)         6242184   \n",
      "                                                                 \n",
      " dense (Dense)               (128, None, 83)           85241     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6410425 (24.45 MB)\n",
      "Trainable params: 6410425 (24.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3a265",
   "metadata": {
    "id": "97c3a265"
   },
   "source": [
    "-------\n",
    "------\n",
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a630910",
   "metadata": {},
   "source": [
    "Let's make sure everything is ok with our model before we spend too much time training! Let's pass in a batch to confirm the model currently predicts random characters without any training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "821f191e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "821f191e",
    "outputId": "4ea65526-7034-4956-ed12-096c4c2b9814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 83) (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "    # predict off some random batch\n",
    "    example_batch_prediction = model(input_example_batch)\n",
    "\n",
    "    # Display off dimentions of the prediction\n",
    "    print(example_batch_prediction.shape, \"(batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffede55a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffede55a",
    "outputId": "2c41b974-274b-4c20-91fa-062e17b59560"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 120, 83), dtype=float32, numpy=\n",
       "array([[[-2.24607605e-02,  1.62957178e-03, -4.22578352e-03, ...,\n",
       "          1.28339359e-03,  1.00502875e-02,  2.78951172e-02],\n",
       "        [-1.04436381e-02,  1.35153988e-02,  1.83796939e-02, ...,\n",
       "         -4.16764207e-02,  2.36079115e-02, -5.68935466e-05],\n",
       "        [-7.86450133e-03,  2.66562365e-02,  6.50447747e-03, ...,\n",
       "         -3.06669492e-02,  2.19769161e-02,  1.14161531e-02],\n",
       "        ...,\n",
       "        [-6.42879307e-03, -1.66745652e-02,  2.50323117e-02, ...,\n",
       "         -1.47083448e-02, -3.85230698e-04,  1.42078698e-02],\n",
       "        [-1.30182505e-02, -1.50342248e-02,  2.10862122e-02, ...,\n",
       "         -2.48284154e-02, -1.68702658e-02,  2.02731285e-02],\n",
       "        [-1.25386408e-02,  6.88271550e-03,  1.01287952e-02, ...,\n",
       "         -2.06914730e-02,  1.86875034e-02,  3.92967202e-02]],\n",
       "\n",
       "       [[-1.26646738e-02, -1.05183898e-02,  1.66758467e-02, ...,\n",
       "          5.89675146e-05, -1.36138396e-02,  7.72566488e-03],\n",
       "        [ 5.57324942e-03, -1.57681154e-03, -1.19575886e-02, ...,\n",
       "         -2.24425104e-02,  2.41831150e-02,  2.14662775e-02],\n",
       "        [ 1.44183380e-03, -1.09047582e-02,  2.99386075e-03, ...,\n",
       "          2.89580156e-03,  1.24819251e-02,  3.88171966e-03],\n",
       "        ...,\n",
       "        [-8.49445409e-04, -3.33995037e-02,  2.27573570e-02, ...,\n",
       "         -2.12760316e-03, -4.76540066e-04,  1.30618019e-02],\n",
       "        [ 1.16909230e-02, -1.76993087e-02, -7.89032225e-03, ...,\n",
       "         -2.47090552e-02,  2.99785323e-02,  2.54884083e-02],\n",
       "        [ 1.62909534e-02, -4.66023979e-04, -3.63843329e-03, ...,\n",
       "          7.91378692e-03,  3.41986977e-02,  1.83508415e-02]],\n",
       "\n",
       "       [[-9.97662172e-03, -6.20691245e-03,  8.62533972e-03, ...,\n",
       "         -1.70348976e-02, -1.58240329e-02,  8.07844568e-03],\n",
       "        [-1.86920650e-02, -1.57205015e-02,  2.22907588e-02, ...,\n",
       "         -8.43862630e-03, -2.02453118e-02,  1.23896310e-02],\n",
       "        [-7.10441964e-03, -3.96507152e-04, -1.17881736e-03, ...,\n",
       "         -4.07077465e-03,  7.41757220e-03,  6.38383068e-03],\n",
       "        ...,\n",
       "        [-7.53082102e-04, -2.13932917e-02,  1.07191661e-02, ...,\n",
       "         -2.29004230e-02, -1.43557778e-02,  4.54438664e-03],\n",
       "        [-8.57471954e-03, -1.32622123e-02,  5.36707463e-04, ...,\n",
       "         -8.58639460e-03,  4.66172677e-03,  6.23443956e-03],\n",
       "        [-1.83483195e-02,  3.13695557e-02, -1.11895381e-02, ...,\n",
       "          2.15656543e-03, -4.41651698e-03,  6.15227735e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.55713928e-02,  1.89153384e-02,  2.73072794e-02, ...,\n",
       "         -3.82260652e-03, -1.46715483e-03,  1.05737112e-02],\n",
       "        [ 5.33816963e-03, -3.32160108e-03,  3.01899239e-02, ...,\n",
       "          9.25784188e-05, -1.34721845e-02,  1.25730103e-02],\n",
       "        [ 2.83359876e-03,  3.63314082e-03,  2.84679397e-03, ...,\n",
       "         -2.02807300e-02,  2.23969221e-02,  1.66252106e-02],\n",
       "        ...,\n",
       "        [-2.59074178e-02, -1.91345997e-02,  1.78588461e-02, ...,\n",
       "          5.65951411e-03, -1.56567816e-03,  1.58779975e-02],\n",
       "        [-8.53770040e-03,  1.14502069e-02,  2.81450786e-02, ...,\n",
       "          3.00237327e-03,  2.13587061e-02,  3.14133093e-02],\n",
       "        [ 1.50994705e-02, -8.74167774e-03,  2.23887190e-02, ...,\n",
       "         -7.42391311e-03,  3.19227763e-02,  1.37927756e-02]],\n",
       "\n",
       "       [[-1.26646738e-02, -1.05183898e-02,  1.66758467e-02, ...,\n",
       "          5.89675146e-05, -1.36138396e-02,  7.72566488e-03],\n",
       "        [-1.88868679e-02, -1.36100361e-02,  2.46296171e-02, ...,\n",
       "          8.42181791e-04, -2.11953782e-02,  1.48653453e-02],\n",
       "        [-2.17572860e-02, -1.42091904e-02,  2.88688205e-02, ...,\n",
       "          1.71532680e-03, -2.53751893e-02,  1.93210095e-02],\n",
       "        ...,\n",
       "        [ 1.04535567e-02, -9.17947385e-03, -7.12695328e-05, ...,\n",
       "          3.94572131e-03, -1.15495399e-02,  4.22189338e-03],\n",
       "        [-1.55773619e-03,  1.17711416e-02, -7.52960658e-03, ...,\n",
       "         -6.82404591e-03,  2.56404076e-02,  3.19682546e-02],\n",
       "        [ 8.79880227e-03,  1.73360836e-02, -1.82976443e-02, ...,\n",
       "         -2.27732956e-02,  1.11717880e-02,  3.03061213e-02]],\n",
       "\n",
       "       [[-1.26646738e-02, -1.05183898e-02,  1.66758467e-02, ...,\n",
       "          5.89675146e-05, -1.36138396e-02,  7.72566488e-03],\n",
       "        [-2.09301244e-02, -2.81521142e-03,  1.89283583e-02, ...,\n",
       "          1.90203954e-02, -9.94884688e-03,  2.99849315e-03],\n",
       "        [ 8.48591328e-03, -1.33787384e-02,  1.30485781e-02, ...,\n",
       "          6.32373709e-03,  1.66694093e-02,  4.75510815e-03],\n",
       "        ...,\n",
       "        [ 1.37492977e-02, -2.95321085e-03,  3.45690502e-03, ...,\n",
       "         -2.23017000e-02, -1.02826785e-02,  1.23139052e-02],\n",
       "        [ 1.24814911e-02,  6.79194834e-03,  1.64445420e-03, ...,\n",
       "          6.88294694e-03,  1.79011486e-02,  1.74158867e-02],\n",
       "        [ 1.83430351e-02,  2.68318523e-02,  2.33398639e-02, ...,\n",
       "          7.15888944e-03,  8.18227604e-03, -2.30895705e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "qbF_kROh1woy",
   "metadata": {
    "id": "qbF_kROh1woy"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_prediction[0], num_samples =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fJEtb25I2PEE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJEtb25I2PEE",
    "outputId": "b0a38519-1622-46b6-ea82-472f27046d1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120, 1), dtype=int64, numpy=\n",
       "array([[11],\n",
       "       [13],\n",
       "       [44],\n",
       "       [59],\n",
       "       [13],\n",
       "       [61],\n",
       "       [82],\n",
       "       [56],\n",
       "       [43],\n",
       "       [21],\n",
       "       [73],\n",
       "       [29],\n",
       "       [ 2],\n",
       "       [15],\n",
       "       [11],\n",
       "       [71],\n",
       "       [69],\n",
       "       [ 7],\n",
       "       [68],\n",
       "       [22],\n",
       "       [66],\n",
       "       [38],\n",
       "       [78],\n",
       "       [25],\n",
       "       [46],\n",
       "       [ 2],\n",
       "       [48],\n",
       "       [18],\n",
       "       [73],\n",
       "       [ 9],\n",
       "       [72],\n",
       "       [12],\n",
       "       [12],\n",
       "       [60],\n",
       "       [75],\n",
       "       [23],\n",
       "       [ 3],\n",
       "       [30],\n",
       "       [ 9],\n",
       "       [57],\n",
       "       [18],\n",
       "       [ 6],\n",
       "       [68],\n",
       "       [81],\n",
       "       [70],\n",
       "       [57],\n",
       "       [82],\n",
       "       [58],\n",
       "       [67],\n",
       "       [51],\n",
       "       [75],\n",
       "       [62],\n",
       "       [69],\n",
       "       [45],\n",
       "       [36],\n",
       "       [13],\n",
       "       [ 6],\n",
       "       [35],\n",
       "       [13],\n",
       "       [69],\n",
       "       [73],\n",
       "       [34],\n",
       "       [70],\n",
       "       [32],\n",
       "       [56],\n",
       "       [48],\n",
       "       [16],\n",
       "       [82],\n",
       "       [46],\n",
       "       [55],\n",
       "       [39],\n",
       "       [37],\n",
       "       [68],\n",
       "       [ 9],\n",
       "       [57],\n",
       "       [49],\n",
       "       [25],\n",
       "       [36],\n",
       "       [58],\n",
       "       [20],\n",
       "       [81],\n",
       "       [36],\n",
       "       [40],\n",
       "       [ 4],\n",
       "       [12],\n",
       "       [14],\n",
       "       [13],\n",
       "       [32],\n",
       "       [19],\n",
       "       [31],\n",
       "       [71],\n",
       "       [38],\n",
       "       [56],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [78],\n",
       "       [ 5],\n",
       "       [21],\n",
       "       [16],\n",
       "       [21],\n",
       "       [80],\n",
       "       [31],\n",
       "       [36],\n",
       "       [61],\n",
       "       [16],\n",
       "       [69],\n",
       "       [ 0],\n",
       "       [72],\n",
       "       [ 1],\n",
       "       [47],\n",
       "       [66],\n",
       "       [59],\n",
       "       [72],\n",
       "       [65],\n",
       "       [62],\n",
       "       [71],\n",
       "       [ 3],\n",
       "       [12],\n",
       "       [60],\n",
       "       [11]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "tUMUuS6E2UTI",
   "metadata": {
    "id": "tUMUuS6E2UTI"
   },
   "outputs": [],
   "source": [
    "# reformat to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "tdoiXOvF2zqs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdoiXOvF2zqs",
    "outputId": "47459e09-f59e-4d07-9023-520279adcd06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 13, 44, 59, 13, 61, 82, 56, 43, 21, 73, 29,  2, 15, 11, 71, 69,\n",
       "        7, 68, 22, 66, 38, 78, 25, 46,  2, 48, 18, 73,  9, 72, 12, 12, 60,\n",
       "       75, 23,  3, 30,  9, 57, 18,  6, 68, 81, 70, 57, 82, 58, 67, 51, 75,\n",
       "       62, 69, 45, 36, 13,  6, 35, 13, 69, 73, 34, 70, 32, 56, 48, 16, 82,\n",
       "       46, 55, 39, 37, 68,  9, 57, 49, 25, 36, 58, 20, 81, 36, 40,  4, 12,\n",
       "       14, 13, 32, 19, 31, 71, 38, 56,  9,  2, 78,  5, 21, 16, 21, 80, 31,\n",
       "       36, 61, 16, 69,  0, 72,  1, 47, 66, 59, 72, 65, 62, 71,  3, 12, 60,\n",
       "       11])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "K9erZbQl22ry",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9erZbQl22ry",
    "outputId": "c81c333e-f8e3-4112-cf54-3f211b00b803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input seq :\n",
      "\n",
      "ncing chips,\n",
      "  O'er whom thy fingers walk with gentle gait,\n",
      "  Making dead wood more blest than living lips,\n",
      "    Since sa\n",
      "\n",
      "\n",
      "Next char Prediction : \n",
      "\n",
      "02Sd2f|aR:rD!40pn)m;kMw?U!W7r-q11et<\"E-b7(mzob|clZtgnTK2(J2nrIoGaW5|U`NLm-bX?Kc9zKO&132G8FpMa-!w':5:yFKf5n\n",
      "q Vkdqjgp\"1e0\n"
     ]
    }
   ],
   "source": [
    "print('Given the input seq :\\n')\n",
    "print(''.join(index_to_char[input_example_batch[0]]))\n",
    "\n",
    "print('\\n')\n",
    "print('Next char Prediction : \\n')\n",
    "print(''.join(index_to_char[sampled_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fzxZpTrtGMtI",
   "metadata": {
    "id": "fzxZpTrtGMtI"
   },
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1igTSS1Gxon",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1igTSS1Gxon",
    "outputId": "900e65e8-895c-44a6-bcf6-7cdb5f30c812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "203/203 [==============================] - 45s 201ms/step - loss: 2.2394\n",
      "Epoch 2/30\n",
      "203/203 [==============================] - 39s 193ms/step - loss: 1.6022\n",
      "Epoch 3/30\n",
      "203/203 [==============================] - 40s 198ms/step - loss: 1.4025\n",
      "Epoch 4/30\n",
      "203/203 [==============================] - 41s 202ms/step - loss: 1.3106\n",
      "Epoch 5/30\n",
      "203/203 [==============================] - 41s 202ms/step - loss: 1.2587\n",
      "Epoch 6/30\n",
      "203/203 [==============================] - 41s 202ms/step - loss: 1.2214\n",
      "Epoch 7/30\n",
      "203/203 [==============================] - 41s 199ms/step - loss: 1.1926\n",
      "Epoch 8/30\n",
      "203/203 [==============================] - 41s 202ms/step - loss: 1.1683\n",
      "Epoch 9/30\n",
      "203/203 [==============================] - 42s 204ms/step - loss: 1.1454\n",
      "Epoch 10/30\n",
      "203/203 [==============================] - 41s 198ms/step - loss: 1.1259\n",
      "Epoch 11/30\n",
      "203/203 [==============================] - 41s 202ms/step - loss: 1.1064\n",
      "Epoch 12/30\n",
      "203/203 [==============================] - 41s 200ms/step - loss: 1.0883\n",
      "Epoch 13/30\n",
      "203/203 [==============================] - 41s 202ms/step - loss: 1.0707\n",
      "Epoch 14/30\n",
      "203/203 [==============================] - 41s 203ms/step - loss: 1.0534\n",
      "Epoch 15/30\n",
      "203/203 [==============================] - 41s 203ms/step - loss: 1.0375\n",
      "Epoch 16/30\n",
      "203/203 [==============================] - 42s 203ms/step - loss: 1.0221\n",
      "Epoch 17/30\n",
      "203/203 [==============================] - 41s 200ms/step - loss: 1.0071\n",
      "Epoch 18/30\n",
      "203/203 [==============================] - 43s 211ms/step - loss: 0.9931\n",
      "Epoch 19/30\n",
      "203/203 [==============================] - 42s 206ms/step - loss: 0.9813\n",
      "Epoch 20/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9701\n",
      "Epoch 21/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9600\n",
      "Epoch 22/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9508\n",
      "Epoch 23/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9436\n",
      "Epoch 24/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9364\n",
      "Epoch 25/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9306\n",
      "Epoch 26/30\n",
      "203/203 [==============================] - 42s 208ms/step - loss: 0.9266\n",
      "Epoch 27/30\n",
      "203/203 [==============================] - 43s 210ms/step - loss: 0.9230\n",
      "Epoch 28/30\n",
      "203/203 [==============================] - 42s 207ms/step - loss: 0.9188\n",
      "Epoch 29/30\n",
      "203/203 [==============================] - 43s 209ms/step - loss: 0.9152\n",
      "Epoch 30/30\n",
      "203/203 [==============================] - 43s 208ms/step - loss: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b817cccfb50>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3GF7Sciuu2g",
   "metadata": {
    "id": "c3GF7Sciuu2g"
   },
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "WPAmXHOjG2Ge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPAmXHOjG2Ge",
    "outputId": "adcc7b30-ac90-485a-bea5-0d6c72fcfb22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('shakespeare_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76N9oqUQu2oq",
   "metadata": {
    "id": "76N9oqUQu2oq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "wHgy61Etu_1w",
   "metadata": {
    "id": "wHgy61Etu_1w"
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size =1)\n",
    "\n",
    "model.load_weights('/content/shakespeare_gen.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aoeSdtide4Ky",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoeSdtide4Ky",
    "outputId": "16a3d6a2-ab44-49f9-8a15-af56a8c1f166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 1000)           83000     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1026)           6242184   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 83)             85241     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6410425 (24.45 MB)\n",
      "Trainable params: 6410425 (24.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "uBa3EgVDe9Ac",
   "metadata": {
    "id": "uBa3EgVDe9Ac"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed, gen_size =1000, temp= 1.0):\n",
    "    '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "\n",
    "  # number of charcters to generate\n",
    "  num_generate = gen_size\n",
    "\n",
    "  # vecotrizing starting seed text\n",
    "  input_eval = [char_to_index[s] for s in start_seed]\n",
    "\n",
    "  # expand to mach batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # empty list to hold resulting  to generated text\n",
    "  text_generate = []\n",
    "\n",
    "\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size ==1\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "    # generate prediction\n",
    "    prediction = model(input_eval)\n",
    "\n",
    "    # remove the batch shape demention\n",
    "    prediction = tf.squeeze(prediction, 0)\n",
    "\n",
    "    # Use a categorical distribution to select the next character\n",
    "    prediction = prediction/ temperature\n",
    "    prediction_id = tf.random.categorical(prediction, num_samples = 1)[-1, 0].numpy()\n",
    "\n",
    "    # pas the prediction character for the next input\n",
    "    input_eval = tf.expand_dims([prediction_id], 0)\n",
    "\n",
    "    # transform back to character letter\n",
    "    text_generate.append(index_to_char[prediction_id])\n",
    "  return (start_seed + ''.join(text_generate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "StvTEUjrjELO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StvTEUjrjELO",
    "outputId": "3c937f5e-577a-48ee-e39e-1fc538f82b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love, if\n",
      "    despite remember.\n",
      "  ESCALUS. Are you to know that?\n",
      "  ESCALUS. Tome all friends kill'd.\n",
      "  PLANTAs d like me quite, whose friend\n",
      "    Deed in the flat to him.\n",
      "  DUKE. He draw the\n",
      "    conquerv.\n",
      "  DUKE. The philosomorsent Mariant mouth ever will return  \n",
      "    Unfold what wild bears  The faces of allegiance fall before his fear,\n",
      "     And you and you it is in truth of a man; be his guilt\n",
      "     To undergo- I assure yet learn to kno?\n",
      "  THIRD WITCH. He went to see.        Exit\n",
      "  ESCALUS. Followed Cassius, Bacchus Late. I' th' studies shall a\n",
      "     was wont to fear. Where will the Duke would the world                                                   Enter SIR NATHANIEL in mortality\n",
      "    To bear with youth where Cawdor doth example.\n",
      "  ESCALUS. What news?\n",
      "  PHOLUS. Pardons me on the Dube; the other\n",
      "    Content. The marketplayes were hard; this man.\n",
      "  MALCOLM. Perchance.\n",
      "  ANGELO. Truly hable to say\n",
      "    Hourself with thy biat- a gentle people\n",
      "    That for their due.\n",
      "  GENTLEWOMAN. Stand clappl\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 'love', gen_size =1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
